{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT large uncased, in its variant _whole word masking_, has been trained over BookCorpus and Wikipedia English with NSP - Next Sentenct Prediction - and MLM - Masked Language Modeling - objectives. Let's import it and its tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking', cache_dir = 'hf_cache')\n",
    "unmasking_model = BertForMaskedLM.from_pretrained('bert-large-uncased-whole-word-masking', cache_dir = 'hf_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline convenience object is created to interface with both the tokenizer and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FillMaskPipeline\n",
    "\n",
    "unmasker = FillMaskPipeline(model = unmasking_model, tokenizer = tokenizer, tokenizer_kwargs = {\"truncation\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following line we are getting all the candidates to the masked word proposed by BERT. Each substitute has a confidence level associated with the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def mask_token(tokens: List[str], idx: int) -> str:\n",
    "    tokens = tokens[:]\n",
    "    tokens[idx] = '[MASK]'\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "original_sentence = 'modern shared room near Harvard.'\n",
    "original_sentence_tokens = tokenizer.tokenize(original_sentence)\n",
    "\n",
    "masked_sentence = mask_token(original_sentence_tokens, 2)\n",
    "candidates = unmasker(masked_sentence, top_k = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, some words can be more suitable than others. We try to figure out the fitness level by reinserting the token into the sentence and by testing the similarity between the original sentence and the one with the mask replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9997) modern shared is near harvard.\n",
      "tensor(0.9992) modern shared campus near harvard.\n",
      "tensor(0.9712) modern shared houses near harvard.\n",
      "tensor(0.9996) modern shared property near harvard.\n",
      "tensor(0.9993) modern shared buildings near harvard.\n",
      "tensor(0.9992) modern shared building near harvard.\n",
      "tensor(0.9996) modern shared house near harvard.\n",
      "tensor(0.9990) modern shared residence near harvard.\n",
      "tensor(0.9984) modern shared, near harvard.\n",
      "tensor(0.9996) modern shared was near harvard.\n",
      "tensor(0.9990) modern shared it near harvard.\n",
      "tensor(0.9704) modern shared housing near harvard.\n",
      "tensor(0.9995) modern shared located near harvard.\n",
      "tensor(0.9802) modern shared space near harvard.\n",
      "tensor(0.9996) modern shared apartments near harvard.\n",
      "tensor(0.9996) modern shared lived near harvard.\n",
      "tensor(0.9994) modern shared school near harvard.\n",
      "tensor(0.9997) modern shared history near harvard.\n",
      "tensor(0.9999) modern shared lives near harvard.\n",
      "tensor(0.9997) modern shared rooms near harvard.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "\n",
    "embeddings_model = BertModel.from_pretrained('bert-large-uncased-whole-word-masking', cache_dir = 'hf_cache')\n",
    "\n",
    "def get_meaned_embeddings(sentence: str):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = embeddings_model(input_ids)\n",
    "        embeddings = outputs.last_hidden_state[0]\n",
    "\n",
    "    return embeddings.mean(1)\n",
    "\n",
    "cos_sim = torch.nn.CosineSimilarity(dim = 0)\n",
    "\n",
    "for candidate in candidates:\n",
    "    sim = cos_sim(get_meaned_embeddings(candidate['sequence']), get_meaned_embeddings(original_sentence))\n",
    "    print(sim, candidate['sequence'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc551502c4709d65c35225dab174e15b6f215f4b9bca0aec7618bac23f51ade6"
  },
  "kernelspec": {
   "display_name": "Python 3.11.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
